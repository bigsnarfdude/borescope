# -*- coding: utf-8 -*-
"""
Toy Models of Superposition - Local Version for Mac M2 Pro

Adapted from the Anthropic "Toy Models of Superposition" paper notebook.
https://github.com/anthropics/toy-models-of-superposition

Usage:
    pip install torch einops plotly pandas matplotlib tqdm
    python toy_models_local.py
"""

import torch
from torch import nn
from torch.nn import functional as F

from typing import Optional
from dataclasses import dataclass
import numpy as np
import einops

from tqdm import trange

import time
import pandas as pd

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

import matplotlib.pyplot as plt
from matplotlib import colors as mcolors
from matplotlib import collections as mc


# =============================================================================
# Device Selection (MPS for M2 Pro)
# =============================================================================

def get_device():
    """Select the best available device: MPS (Apple Silicon), CUDA, or CPU."""
    if torch.backends.mps.is_available():
        print("Using MPS (Apple Silicon)")
        return 'mps'
    elif torch.cuda.is_available():
        print("Using CUDA")
        return 'cuda'
    else:
        print("Using CPU")
        return 'cpu'

DEVICE = get_device()


# =============================================================================
# Model Configuration and Architecture
# =============================================================================

@dataclass
class Config:
    n_features: int
    n_hidden: int
    # We optimize n_instances models in a single training loop
    # to let us sweep over sparsity or importance curves efficiently.
    n_instances: int


class Model(nn.Module):
    def __init__(
        self,
        config: Config,
        feature_probability: Optional[torch.Tensor] = None,
        importance: Optional[torch.Tensor] = None,
        device: str = 'cpu'
    ):
        super().__init__()
        self.config = config
        self.W = nn.Parameter(
            torch.empty((config.n_instances, config.n_features, config.n_hidden), device=device)
        )
        nn.init.xavier_normal_(self.W)
        self.b_final = nn.Parameter(
            torch.zeros((config.n_instances, config.n_features), device=device)
        )

        if feature_probability is None:
            feature_probability = torch.ones(())
        self.feature_probability = feature_probability.to(device)
        
        if importance is None:
            importance = torch.ones(())
        self.importance = importance.to(device)

    def forward(self, features):
        # features: [..., instance, n_features]
        # W: [instance, n_features, n_hidden]
        hidden = torch.einsum("...if,ifh->...ih", features, self.W)
        out = torch.einsum("...ih,ifh->...if", hidden, self.W)
        out = out + self.b_final
        out = F.relu(out)
        return out

    def generate_batch(self, n_batch: int):
        feat = torch.rand(
            (n_batch, self.config.n_instances, self.config.n_features),
            device=self.W.device
        )
        batch = torch.where(
            torch.rand(
                (n_batch, self.config.n_instances, self.config.n_features),
                device=self.W.device
            ) <= self.feature_probability,
            feat,
            torch.zeros((), device=self.W.device),
        )
        return batch


# =============================================================================
# Learning Rate Schedules
# =============================================================================

def linear_lr(step, steps):
    return (1 - (step / steps))

def constant_lr(*_):
    return 1.0

def cosine_decay_lr(step, steps):
    return np.cos(0.5 * np.pi * step / (steps - 1))


# =============================================================================
# Training Loop
# =============================================================================

def optimize(
    model: Model,
    n_batch: int = 1024,
    steps: int = 10_000,
    print_freq: int = 100,
    lr: float = 1e-3,
    lr_scale=constant_lr,
    hooks: list = []
):
    cfg = model.config
    opt = torch.optim.AdamW(list(model.parameters()), lr=lr)

    start = time.time()
    with trange(steps) as t:
        for step in t:
            step_lr = lr * lr_scale(step, steps)
            for group in opt.param_groups:
                group['lr'] = step_lr
            
            opt.zero_grad(set_to_none=True)
            batch = model.generate_batch(n_batch)
            out = model(batch)
            error = (model.importance * (batch.abs() - out) ** 2)
            loss = einops.reduce(error, 'b i f -> i', 'mean').sum()
            loss.backward()
            opt.step()

            if hooks:
                hook_data = dict(
                    model=model,
                    step=step,
                    opt=opt,
                    error=error,
                    loss=loss,
                    lr=step_lr
                )
                for h in hooks:
                    h(hook_data)
            
            if step % print_freq == 0 or (step + 1 == steps):
                t.set_postfix(
                    loss=loss.item() / cfg.n_instances,
                    lr=step_lr,
                )
    
    elapsed = time.time() - start
    print(f"Training completed in {elapsed:.2f}s")


# =============================================================================
# Visualization Functions
# =============================================================================

def plot_intro_diagram(model: Model, save_path: str = None):
    """Plot the introduction diagram showing feature representations."""
    cfg = model.config
    WA = model.W.detach()
    
    plt.rcParams["axes.prop_cycle"] = plt.cycler(
        "color", plt.cm.viridis(model.importance[0].cpu().numpy())
    )
    plt.rcParams['figure.dpi'] = 200
    
    sel = range(cfg.n_instances)
    fig, axs = plt.subplots(1, len(sel), figsize=(2 * len(sel), 2))
    
    for i, ax in zip(sel, axs):
        W = WA[i].cpu().detach().numpy()
        colors = [
            mcolors.to_rgba(c)
            for c in plt.rcParams['axes.prop_cycle'].by_key()['color']
        ]
        ax.scatter(W[:, 0], W[:, 1], c=colors[0:len(W[:, 0])])
        ax.set_aspect('equal')
        ax.add_collection(
            mc.LineCollection(np.stack((np.zeros_like(W), W), axis=1), colors=colors)
        )

        z = 1.5
        ax.set_facecolor('#FCFBF8')
        ax.set_xlim((-z, z))
        ax.set_ylim((-z, z))
        ax.tick_params(
            left=True, right=False, labelleft=False,
            labelbottom=False, bottom=True
        )
        for spine in ['top', 'right']:
            ax.spines[spine].set_visible(False)
        for spine in ['bottom', 'left']:
            ax.spines[spine].set_position('center')
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=200, bbox_inches='tight')
        print(f"Saved figure to {save_path}")
    plt.show()


def render_features(model: Model, which=np.s_[:]):
    """Render feature visualizations using Plotly."""
    cfg = model.config
    W = model.W.detach()
    W_norm = W / (1e-5 + torch.linalg.norm(W, 2, dim=-1, keepdim=True))

    interference = torch.einsum('ifh,igh->ifg', W_norm, W)
    interference[:, torch.arange(cfg.n_features), torch.arange(cfg.n_features)] = 0

    polysemanticity = torch.linalg.norm(interference, dim=-1).cpu()
    norms = torch.linalg.norm(W, 2, dim=-1).cpu()
    WtW = torch.einsum('sih,soh->sio', W, W).cpu()

    x = torch.arange(cfg.n_features)
    width = 0.9

    which_instances = np.arange(cfg.n_instances)[which]
    fig = make_subplots(
        rows=len(which_instances),
        cols=2,
        shared_xaxes=True,
        vertical_spacing=0.02,
        horizontal_spacing=0.1
    )
    
    for (row, inst) in enumerate(which_instances):
        fig.add_trace(
            go.Bar(
                x=x,
                y=norms[inst],
                marker=dict(color=polysemanticity[inst], cmin=0, cmax=1),
                width=width,
            ),
            row=1 + row, col=1
        )
        data = WtW[inst].numpy()
        fig.add_trace(
            go.Image(
                z=plt.cm.coolwarm((1 + data) / 2, bytes=True),
                colormodel='rgba256',
                customdata=data,
                hovertemplate='In: %{x}<br>Out: %{y}<br>Weight: %{customdata:0.2f}'
            ),
            row=1 + row, col=2
        )

    fig.add_vline(
        x=(x[cfg.n_hidden - 1] + x[cfg.n_hidden]) / 2,
        line=dict(width=0.5),
        col=1,
    )

    fig.update_layout(
        showlegend=False,
        width=600,
        height=100 * len(which_instances),
        margin=dict(t=0, b=0)
    )
    fig.update_xaxes(visible=False)
    fig.update_yaxes(visible=False)
    return fig


@torch.no_grad()
def compute_dimensionality(W):
    """Compute the dimensionality fractions for feature geometry analysis."""
    norms = torch.linalg.norm(W, 2, dim=-1)
    W_unit = W / torch.clamp(norms[:, :, None], 1e-6, float('inf'))
    interferences = (torch.einsum('eah,ebh->eab', W_unit, W) ** 2).sum(-1)
    dim_fracs = (norms ** 2 / interferences)
    return dim_fracs.cpu()


def plot_feature_geometry(model: Model):
    """Plot feature geometry analysis."""
    dim_fracs = compute_dimensionality(model.W)
    density = model.feature_probability[:, 0].cpu()
    W = model.W.detach()

    fig = go.Figure()

    # Add horizontal reference lines
    for a, b in [(1, 2), (2, 3), (2, 5), (2, 6), (2, 7)]:
        val = a / b
        fig.add_hline(val, line_color="purple", opacity=0.2, annotation=dict(text=f"{a}/{b}"))

    for a, b in [(5, 6), (4, 5), (3, 4), (3, 8), (3, 12), (3, 20)]:
        val = a / b
        fig.add_hline(val, line_color="blue", opacity=0.2, annotation=dict(text=f"{a}/{b}", x=0.05))

    # Plot dimensionality fractions
    for i in range(len(W)):
        fracs_ = dim_fracs[i]
        N = fracs_.shape[0]
        xs = 1 / density
        if i != len(W) - 1:
            dx = xs[i + 1] - xs[i]
        else:
            dx = 0
        fig.add_trace(
            go.Scatter(
                x=1 / density[i] * np.ones(N) + dx * np.random.uniform(-0.1, 0.1, N),
                y=fracs_,
                marker=dict(color='black', size=1, opacity=0.5),
                mode='markers',
            )
        )

    fig.update_xaxes(type='log', title='1/(1-S)', showgrid=False)
    fig.update_yaxes(showgrid=False)
    fig.update_layout(showlegend=False, title="Feature Geometry")
    return fig


# =============================================================================
# Main Experiments
# =============================================================================

def run_intro_experiment():
    """Run the introduction figure experiment (5 features, 2 hidden)."""
    print("\n" + "=" * 60)
    print("Running Introduction Experiment")
    print("=" * 60)
    
    config = Config(
        n_features=5,
        n_hidden=2,
        n_instances=10,
    )

    model = Model(
        config=config,
        device=DEVICE,
        importance=(0.9 ** torch.arange(config.n_features))[None, :],
        feature_probability=(20 ** -torch.linspace(0, 1, config.n_instances))[:, None]
    )

    optimize(model)
    plot_intro_diagram(model, save_path="intro_diagram.png")
    return model


def run_features_experiment():
    """Run the feature visualization experiment (100 features, 20 hidden)."""
    print("\n" + "=" * 60)
    print("Running Features Visualization Experiment")
    print("=" * 60)
    
    config = Config(
        n_features=100,
        n_hidden=20,
        n_instances=20,
    )

    model = Model(
        config=config,
        device=DEVICE,
        importance=(100 ** -torch.linspace(0, 1, config.n_features))[None, :],
        feature_probability=(20 ** -torch.linspace(0, 1, config.n_instances))[:, None]
    )

    optimize(model)
    
    fig = render_features(model, np.s_[::2])
    fig.write_html("features_visualization.html")
    print("Saved features visualization to features_visualization.html")
    fig.show()
    return model


def run_geometry_experiment():
    """Run the feature geometry experiment (200 features, 20 hidden)."""
    print("\n" + "=" * 60)
    print("Running Feature Geometry Experiment")
    print("=" * 60)
    
    config = Config(
        n_features=200,
        n_hidden=20,
        n_instances=20,
    )

    model = Model(
        config=config,
        device=DEVICE,
        feature_probability=(20 ** -torch.linspace(0, 1, config.n_instances))[:, None]
    )

    optimize(model)
    
    # Frobenius norm analysis
    fig1 = px.line(
        x=1 / model.feature_probability[:, 0].cpu(),
        y=(model.config.n_hidden / (torch.linalg.matrix_norm(model.W.detach(), 'fro') ** 2)).cpu(),
        log_x=True,
        markers=True,
        title="Frobenius Norm Analysis"
    )
    fig1.update_xaxes(title="1/(1-S)")
    fig1.update_yaxes(title=f"m/||W||_F^2")
    fig1.write_html("frobenius_analysis.html")
    print("Saved Frobenius analysis to frobenius_analysis.html")
    fig1.show()
    
    # Feature geometry
    fig2 = plot_feature_geometry(model)
    fig2.write_html("feature_geometry.html")
    print("Saved feature geometry to feature_geometry.html")
    fig2.show()
    
    return model


# =============================================================================
# Entry Point
# =============================================================================

if __name__ == "__main__":
    print("Toy Models of Superposition - Local Version")
    print(f"Device: {DEVICE}")
    print(f"PyTorch version: {torch.__version__}")
    
    # Run all experiments
    model_intro = run_intro_experiment()
    model_features = run_features_experiment()
    model_geometry = run_geometry_experiment()
    
    print("\n" + "=" * 60)
    print("All experiments completed!")
    print("=" * 60)
