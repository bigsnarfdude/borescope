{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Steering Experiment Analysis\n",
    "\n",
    "Analyzing results from the Eiffel Tower Llama reproduction experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment(log_folder):\n",
    "    \"\"\"Load results from a sweep experiment.\"\"\"\n",
    "    results_path = Path(log_folder) / 'results.json'\n",
    "    if results_path.exists():\n",
    "        with open(results_path) as f:\n",
    "            return pd.DataFrame(json.load(f))\n",
    "    return None\n",
    "\n",
    "# Find all log folders\n",
    "log_dir = Path('../logs')\n",
    "if log_dir.exists():\n",
    "    experiments = sorted(log_dir.glob('*'))\n",
    "    print(f\"Found {len(experiments)} experiment folders:\")\n",
    "    for exp in experiments:\n",
    "        print(f\"  - {exp.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_harmonic_mean(concept, instruction, fluency):\n",
    "    \"\"\"Compute harmonic mean of three LLM metrics.\"\"\"\n",
    "    if concept * instruction * fluency < 1e-6:\n",
    "        return 0.0\n",
    "    return 3.0 / (1.0/concept + 1.0/instruction + 1.0/fluency)\n",
    "\n",
    "def plot_sweep_results(df, title=\"Steering Sweep Results\"):\n",
    "    \"\"\"Plot metrics vs steering intensity.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "    \n",
    "    # Group by steering intensity and compute stats\n",
    "    grouped = df.groupby('steering_intensity').agg({\n",
    "        'llm_score_concept': ['mean', 'std'],\n",
    "        'llm_score_instruction': ['mean', 'std'],\n",
    "        'llm_score_fluency': ['mean', 'std'],\n",
    "        'avg_log_prob': ['mean', 'std'],\n",
    "        'rep3': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "    \n",
    "    x = grouped['steering_intensity']\n",
    "    \n",
    "    # LLM metrics\n",
    "    for ax, metric, label in zip(\n",
    "        axes[0],\n",
    "        ['llm_score_concept', 'llm_score_instruction', 'llm_score_fluency'],\n",
    "        ['Concept Inclusion', 'Instruction Following', 'Fluency']\n",
    "    ):\n",
    "        mean = grouped[(metric, 'mean')]\n",
    "        std = grouped[(metric, 'std')]\n",
    "        ax.plot(x, mean, 'o-', label=label)\n",
    "        ax.fill_between(x, mean - std, mean + std, alpha=0.2)\n",
    "        ax.set_xlabel('Steering Intensity (α)')\n",
    "        ax.set_ylabel('Score (0-2)')\n",
    "        ax.set_title(label)\n",
    "        ax.set_ylim(-0.1, 2.1)\n",
    "    \n",
    "    # Auxiliary metrics\n",
    "    ax = axes[1, 0]\n",
    "    mean = grouped[('avg_log_prob', 'mean')]\n",
    "    std = grouped[('avg_log_prob', 'std')]\n",
    "    ax.plot(x, -mean, 'o-')  # Negative for \"surprise\"\n",
    "    ax.fill_between(x, -(mean - std), -(mean + std), alpha=0.2)\n",
    "    ax.set_xlabel('Steering Intensity (α)')\n",
    "    ax.set_ylabel('Surprise (neg log prob)')\n",
    "    ax.set_title('Surprise in Reference Model')\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    mean = grouped[('rep3', 'mean')]\n",
    "    std = grouped[('rep3', 'std')]\n",
    "    ax.plot(x, mean, 'o-')\n",
    "    ax.fill_between(x, mean - std, mean + std, alpha=0.2)\n",
    "    ax.set_xlabel('Steering Intensity (α)')\n",
    "    ax.set_ylabel('Rep3 Fraction')\n",
    "    ax.set_title('3-gram Repetition')\n",
    "    \n",
    "    # Harmonic mean\n",
    "    ax = axes[1, 2]\n",
    "    if 'llm_score_concept' in df.columns and df['llm_score_concept'].notna().any():\n",
    "        df['harmonic_mean'] = df.apply(\n",
    "            lambda r: compute_harmonic_mean(\n",
    "                r['llm_score_concept'] or 0,\n",
    "                r['llm_score_instruction'] or 0, \n",
    "                r['llm_score_fluency'] or 0\n",
    "            ), axis=1)\n",
    "        hm_grouped = df.groupby('steering_intensity')['harmonic_mean'].agg(['mean', 'std']).reset_index()\n",
    "        ax.plot(hm_grouped['steering_intensity'], hm_grouped['mean'], 'o-')\n",
    "        ax.fill_between(hm_grouped['steering_intensity'], \n",
    "                       hm_grouped['mean'] - hm_grouped['std'],\n",
    "                       hm_grouped['mean'] + hm_grouped['std'], alpha=0.2)\n",
    "    ax.set_xlabel('Steering Intensity (α)')\n",
    "    ax.set_ylabel('Harmonic Mean')\n",
    "    ax.set_title('Harmonic Mean of LLM Scores')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp01: Baseline Sweep Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load exp01 results (update path after running)\n",
    "# df_exp01 = load_experiment('../logs/YYYYMMDD-HHMMSS_sweep_1D')\n",
    "# if df_exp01 is not None:\n",
    "#     plot_sweep_results(df_exp01, \"Exp01: Baseline Steering Sweep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp02: Clamping Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clamping(df_add, df_clamp):\n",
    "    \"\"\"Compare additive vs clamping steering.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    \n",
    "    for df, label, color in [(df_add, 'Additive', 'blue'), (df_clamp, 'Clamping', 'red')]:\n",
    "        grouped = df.groupby('steering_intensity').agg({\n",
    "            'llm_score_concept': 'mean',\n",
    "            'llm_score_fluency': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        axes[0].plot(grouped['steering_intensity'], grouped['llm_score_concept'], \n",
    "                    'o-', label=label, color=color)\n",
    "        axes[1].plot(grouped['steering_intensity'], grouped['llm_score_fluency'],\n",
    "                    'o-', label=label, color=color)\n",
    "        \n",
    "        # Harmonic mean\n",
    "        df['hm'] = df.apply(lambda r: compute_harmonic_mean(\n",
    "            r['llm_score_concept'] or 0, r['llm_score_instruction'] or 0, r['llm_score_fluency'] or 0\n",
    "        ), axis=1)\n",
    "        hm_grouped = df.groupby('steering_intensity')['hm'].mean().reset_index()\n",
    "        axes[2].plot(hm_grouped['steering_intensity'], hm_grouped['hm'],\n",
    "                    'o-', label=label, color=color)\n",
    "    \n",
    "    axes[0].set_title('Concept Inclusion')\n",
    "    axes[1].set_title('Fluency')\n",
    "    axes[2].set_title('Harmonic Mean')\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Steering Intensity (α)')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.suptitle('Exp02: Clamping vs Additive Steering')\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df):\n",
    "    \"\"\"Plot correlation matrix between metrics.\"\"\"\n",
    "    metrics = ['llm_score_concept', 'llm_score_instruction', 'llm_score_fluency',\n",
    "               'avg_log_prob', 'rep3']\n",
    "    \n",
    "    # Filter to only metrics that exist\n",
    "    metrics = [m for m in metrics if m in df.columns and df[m].notna().any()]\n",
    "    \n",
    "    if len(metrics) > 0:\n",
    "        corr = df[metrics].corr()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.heatmap(corr, annot=True, cmap='RdBu_r', center=0, \n",
    "                   vmin=-1, vmax=1, ax=ax)\n",
    "        ax.set_title('Metric Correlation Matrix')\n",
    "        return fig\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_outputs(df, steering_intensities=[0, 5, 8, 12]):\n",
    "    \"\"\"Show sample outputs at different steering intensities.\"\"\"\n",
    "    for intensity in steering_intensities:\n",
    "        # Find closest intensity\n",
    "        closest = df.iloc[(df['steering_intensity'] - intensity).abs().argsort()[:1]]\n",
    "        if len(closest) > 0:\n",
    "            row = closest.iloc[0]\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Steering Intensity: {row['steering_intensity']:.1f}\")\n",
    "            print(f\"Prompt: {row['prompt'][:100]}...\")\n",
    "            print(f\"\\nAnswer: {row['answer'][:500]}...\")\n",
    "            if row.get('llm_score_concept') is not None:\n",
    "                print(f\"\\nScores: C={row['llm_score_concept']}, I={row['llm_score_instruction']}, F={row['llm_score_fluency']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings Summary\n",
    "\n",
    "After running experiments, document key findings here:\n",
    "\n",
    "### 1. Optimal Steering Coefficient\n",
    "- Paper claims α ≈ 8.5 for layer 15 feature #21576\n",
    "- Our finding: TBD\n",
    "\n",
    "### 2. Clamping vs Additive\n",
    "- Paper claims clamping improves concept inclusion\n",
    "- Our finding: TBD\n",
    "\n",
    "### 3. Multi-Feature Steering\n",
    "- Paper claims marginal improvement\n",
    "- Our finding: TBD\n",
    "\n",
    "### 4. Generation Parameters\n",
    "- Paper claims temp=0.5 + rep_penalty=1.1 helps\n",
    "- Our finding: TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
